# LLM Inference & Optimization Platform

A production-grade LLM inference platform with:
- Multi-model serving
- Semantic routing
- Caching
- Observability
- Load testing with Locust