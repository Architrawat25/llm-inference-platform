# LLM Inference & Optimization Platform

A production-grade LLM inference platform with:
- Multi-model serving
- Semantic routing
- Caching
- Observability
- Load testing with Locust



A production-grade backend system for Large Language Model inference that demonstrates semantic routing, multi-model orchestration.

This project designed to showcase systems engineering, ML infrastructure design, and performance evaluation similar to internal services used in real-world AI platforms.

---
## Models I have used
I have used Hugging Face MLX models **(specifically optimized to run efficiently on Apple Silicon)**

- **Small model:** [Llama-3.2-3B-Instruct-4bit](https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-4bit) (1.8GB)
- **Large model:** [Meta-Llama-3.1-8B-Instruct-4bit](https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-4bit) (4.5GB)
---

## Project Highlights

* **Multi-model inference**
* **Semantic routing** using sentence embeddings
* **Clean model abstraction layer** (router and API are model-agnostic)
* **FastAPI backend**
* **Lifecycle-managed model loading (lifespan)**
* **End-to-end latency measurement**
#### Currently working on:
* **Load testing with Locust** using realistic traffic patterns
* **Caching** using redis

---

## ğŸ§  System Architecture (High Level)

```
Client
  â”‚
  â–¼
FastAPI API Layer
  â”‚
  â–¼
Semantic Router (Intent + Similarity)
  â”‚
  â”œâ”€â”€ Small Model (fast, cheap)
  â””â”€â”€ Large Model (slow, high-quality)
```

**The API does not know which model is used.** Routing decisions are made dynamically based on semantic similarity.

---

## ğŸ“‚ Project Structure

```
LLM-Inference-Platform/
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py          # FastAPI app + lifespan
â”‚   â”œâ”€â”€ routes.py        # Inference endpoint
â”‚   â””â”€â”€ schemas.py       # Request / response / error models
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base.py          # Model interface
â”‚   â”œâ”€â”€ small_model.py   # Lightweight model
â”‚   â””â”€â”€ large_model.py   # Large model
â”‚
â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ embedder.py      # Sentence embedding model
â”‚   â”œâ”€â”€ intents.py       # Intent definitions + embeddings
â”‚   â””â”€â”€ router.py        # Semantic routing logic
â”œâ”€â”€ tests                # planned
â”œâ”€â”€ cache                # planned
â”œâ”€â”€ observablity         # planned
â”œâ”€â”€ .env.example         # Environment variable template
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## Semantic Routing

Routing is performed using **sentence embeddings**:

1. Prompt â†’ embedding
2. Compare against predefined intent embeddings
3. Compute cosine similarity
4. Route to:

   * **Small model** â†’ simple / casual prompts
   * **Large model** â†’ technical / complex prompts

Routing threshold is configurable via environment variables.

---

## âš™ï¸ Tech Stack

* **Backend**: FastAPI
* **Models**:
  * Hugging Face Transformers
* **Embeddings**: Sentence-Transformers (`all-MiniLM-L6-v2`)
* **Routing**: Cosine similarityâ€“based semantic routing
* **Load Testing**: Locust
* **Runtime**: Python 3.11

---

## ğŸ§ª Running the API Locally

### 1ï¸âƒ£ Create and activate virtual environment

```bash
python -m venv env
source env/bin/activate
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Create `.env` file

```env
EMBEDDING_MODEL_NAME=all-MiniLM-L6-v2
SMALL_MODEL_NAME=
ROUTING_THRESHOLD=
```

### 4ï¸âƒ£ Start the API

```bash
uvicorn api.main:app --reload --env-file .env
```

### 5ï¸âƒ£ Test health endpoint

```bash
curl http://127.0.0.1:8000/health
```

---

## ğŸ“¡ Inference API

### Endpoint

```
POST /generate
```

### Request

```json
{
  "prompt": "Explain transformers in simple terms",
  "max_tokens": 150
}
```

### Response

```json
{
  "response": "...generated text...",
  "model_used": "large_model",
  "latency_ms": 842.3,
  "intent": "technical",
  "score": 0.71,
  "cached": false
}
```

---

## ğŸ”® Future Improvements
* Load-Testing with Locust
* Redis-based response caching
* Metrics & observability
* Dockerized deployment

---
## ğŸ“œ License

MIT License
